# Phase III: Prototypes and User Testing

## Introduction

In the last report, the UX team ended with Cognitive Walkthroughs and Informal Feedbacks and the team was wondering whether the UX team could test our designs to make sure that our product would be easy to use. The main goal of this sprint was to learn how to make and use prototypes as a workable model of the extension and to gather data from participants.

## Methods

[Usability test protocol PDF!](protocol.pdf)

For this sprint, the UX team explored one more method in order to get a better understanding of what to improve for the extension. This method being **User Tests**. For the User Tests, the UX team had to do a lot of setup. The UX team first had to do training to be able to do the User Tests in the first place. Then the team had to create protocols that the team goes through in the User Test. Finally, before we started, the team made consent forms for each of the participants in order to get their consent to be a participant. After all that, to start up, the team first talked about some protocols that would be followed by the participants, ensuring them that their name and other private information would be shredded after the tests are over. Then the team started with getting some background information from each of the participants. From there, the UX team began the tests. Each participant ran through four different tests to compare the extension's competitor and the extension itself. During each test, the UX team took notes of each step the participant was doing and then asked if the step was completed or not. After the tests, participants were asked to rate the extension compared to the extension's competitor to get an idea of what can be improved upon.

## Findings

[Spreadsheet of our results!](https://docs.google.com/spreadsheets/d/13pTX2hT4gQKe3u2hVPQc0UNdrYdhTEd_dF8wrpG-Pyk/edit?usp=sharing)

From our user tests, we were able to get valuable user data and feature suggestions.  The previous experiences for the users with other price saving tools was generally negative, with most claiming they often wouldn’t work.  This shows that there is definitely a need for working apps like PriceCheck.  During the tests against competitor Honey, users identified major issues with the search engine’s ability.  All of the users had difficulties finding the item that they were supposed to, as the search results were specific enough to their search.  They found that they would have to search through results that seemed to not be arranged in any useful order.  After this, users also generally had difficulty recognizing Honey’s “Droplist” icon when setting an alert for the item.  Because of the lack of labeling to the icon, some users struggled to find it, with one third of users not being able to complete the task.  For both finding an item and creating an alert, users gave Honey a mean score of 3.5 out of 5.

When testing the PriceCheck’s prototype, users appreciated the simpler layout, which was shown by less confusion when navigating the app.  However, some users felt that the item page could have had more information in the item descriptions, such as prices and images of the listings.  Users also expressed that there could have been some confusion between the “Best Brand New Price” and “Best Price”, the latter of which would be more appropriate to call “Best Used Price”.  While creating an alert, one user pointed out that the current system doesn’t have an easy way to look for any price drop, and requires them to have the current price known to them already.  Overall, users gave PriceCheck’s both ease of use scores for finding an item and creating an alert a 4.7 out of 5.  During the debrief, when we asked users to compare PriceCheck to Honey, on average, users found that PriceCheck was a more efficient app, however, when rating satisfaction, there was a greater spread across the results, as some felt that Honey’s more informational item descriptions made them feel like they were getting the best prices.

Whisker plots for ease of use scores (out of 5) for finding an item, PriceCheck in purple and Honey in orange.

![finding-item](finding-item.png)

Whisker plots for ease of use scores for creating an alert.

![creating-alert](creating-alert.png)

Whisker plots for Pricecheck’s ease of use, efficiency, and satisfaction scores (out of 5) when compared to Honey.  Ease of use in green, efficiency in blue, and satisfaction in red.

![comparisons](comparisons.png)

## Conclusions

From the insights that the users were able to highlight the demand and potential for PriceCheck in the market of price-saving tools.  Users generally had negative experiences with competition in the field such as Honey, citing the weak search and unclear interface.  PriceCheck’s clear layout and simple design were beneficial to user interactions, and made navigation much easier for a newcomer to the app.  The comparison with Honey allowed for users to point out some valuable features missing from the app, such as displaying the prices and images of the listings on the item page, and modifying the alert creation to default at certain prices.  Future iterations of PriceCheck will have to take these features into consideration while also balancing the simplicity and efficiency focused design of the app.

## Caveats

Caveats for the extension included the fact that it was not a fully developed application, which limited the amount of testing the UX team could conduct. Another caveat was the small sample size of participants a larger sample size would have been more optimal.
